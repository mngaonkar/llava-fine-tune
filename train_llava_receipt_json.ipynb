{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b467d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from  transformers import AutoTokenizer, TrainingArguments, BitsAndBytesConfig, AutoProcessor, LlavaForConditionalGeneration\n",
    "import bitsandbytes as bnb\n",
    "import numpy as np\n",
    "import torch\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.utils.data import Dataset, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48778925",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "MODEL_NAME = os.getenv(\"MODEL_NAME\")\n",
    "MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\"))\n",
    "OUTPUT_SIZE = (384, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66133ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llava 1.5, images are 336x336\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME,\n",
    "                                         torch_dtype=torch.float16,\n",
    "                                         use_auth_token=True)\n",
    "processor.tokenizer.padding_side = \"right\" # always on right for training\n",
    "# if processor.tokenizer.chat_template is None:\n",
    "#     print(\"Setting chat template for processor\")\n",
    "#     processor.tokenizer.chat_template = (\n",
    "#         \"{% for message in messages %}\"\n",
    "#         \"{% if message['role'] == 'user' %}\"\n",
    "#         \"USER: {{ message['content'] }}\\n\"\n",
    "#         \"{% elif message['role'] == 'assistant' %}\"\n",
    "#         \"ASSISTANT: {{ message['content'] }}\\n\"\n",
    "#         \"{% endif %}\"\n",
    "#         \"{% endfor %}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c084809",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LORA = False\n",
    "USE_QLORA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c728e0c",
   "metadata": {},
   "source": [
    "### Load Model \n",
    "Load model from HuggingFace with 4 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA or USE_QLORA:\n",
    "    if USE_QLORA:\n",
    "        print(\"Using QLoRA\")\n",
    "        # Load the model with 4-bit quantization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using LoRA\")\n",
    "        bnb_config = None\n",
    "        \n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Using full precision\")\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7cee18",
   "metadata": {},
   "source": [
    "### Performance Efficient Fine Tuning (PEFT)\n",
    "\n",
    "Add adapter to all linear layers of the model except multi_modal_projector and vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = [\"multi_modal_projector\", \"vision_model\"]\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "\n",
    "    return list(lora_module_names)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    target_modules=find_all_linear_names(model),\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7810c-cdac-43bc-8f06-2e8f033d00c8",
   "metadata": {},
   "source": [
    "### Download reciept data for training \n",
    "The objective is to scan the receipt and convert the content to JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7910c5-3733-4173-95c5-de44abb6e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"naver-clova-ix/cord-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f2f3cc-d9e9-4922-9d14-9cb7b666f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce0bb8-5f8f-444f-9b00-61866c861d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(load_dataset(\"naver-clova-ix/cord-v2\", split=\"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef42cfe-2a44-40af-b756-ed2a40dffe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1801a7af-2ef4-4189-889d-161b8fb8d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = json.loads(dataset[\"train\"][0][\"ground_truth\"])[\"gt_parse\"]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c86dab-c883-443e-8971-408cbca97718",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0][\"image\"].resize((400, 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, dataset_name:str, split = \"train\", transform=None):\n",
    "        self.data = load_dataset(dataset_name, split=split)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        sample = self.data[idx]\n",
    "        image = sample[\"image\"]\n",
    "\n",
    "        # Load the image\n",
    "        try:\n",
    "            image = image.convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading image {image_path}: {e}\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Load the text\n",
    "        text = json.dumps(json.loads(sample[\"ground_truth\"])[\"gt_parse\"])\n",
    "\n",
    "        return image, text\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ded10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataset class\n",
    "from torchvision import transforms\n",
    "\n",
    "output_size = OUTPUT_SIZE  # Define the output size for resizing\n",
    "transform = None\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(output_size),\n",
    "        # transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "dataset = CustomImageDataset(\n",
    "    dataset_name=\"naver-clova-ix/cord-v2\",\n",
    "    split=\"train\",\n",
    "    transform=transform,\n",
    ")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"First sample: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656793ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Validation size: {len(val_dataset)}\")\n",
    "\n",
    "print(\"Train dataset sample:\", train_dataset[0])\n",
    "print(\"Validation dataset sample:\", val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset image shape:\", train_dataset[0][0])\n",
    "print(\"Train dataset text shape:\", train_dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset\n",
    "def preprocess_image(image_path, output_size=(224, 224)):\n",
    "    from PIL import Image\n",
    "    from torchvision import transforms\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(output_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf9e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(examples):\n",
    "    \"\"\" Collate function to process a batch of images and texts.\n",
    "    Args:\n",
    "        batch (list): List of tuples containing image tensors and text strings.\n",
    "        Returns:\n",
    "            dict: Dictionary containing processed images and tokenized texts.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    texts = []\n",
    "\n",
    "    print(f\"Number of examples: {len(examples)}\")\n",
    "    for example in examples:\n",
    "        image, label = example\n",
    "        if not isinstance(image, Image.Image):\n",
    "            raise ValueError(f\"Expected PIL Image, got {type(image)}\")\n",
    "        \n",
    "        images.append(image)\n",
    "        # messages = [\n",
    "        #     {\"role\": \"user\", \"content\": \"<image>\"},\n",
    "        #     {\"role\": \"assistant\", \"content\": label}\n",
    "        # ]\n",
    "        \n",
    "        # prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        \n",
    "        prompt = \"USER: <image>\\n Extract JSON\\n\" + \"ASSISTANT: \" + label\n",
    "\n",
    "        # DEBUG: START\n",
    "        print(f\"Train prompt: {prompt}\")\n",
    "        # Check if the prompt contains the <image> token\n",
    "        tokenized_prompt = processor.tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        image_token_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
    "        num_image_tokens_in_prompt = sum(1 for token in tokenized_prompt if token == image_token_id)\n",
    "        print(f\"Number of <image> tokens in prompt: {num_image_tokens_in_prompt}\")\n",
    "        # DEBUG: END\n",
    "\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        texts.append(prompt)\n",
    "    \n",
    "    batch = processor(text=texts, \n",
    "                      images=images, \n",
    "                      padding=True, \n",
    "                    #   truncation=True, \n",
    "                    #   max_length=MAX_LENGTH, \n",
    "                      return_tensors=\"pt\")\n",
    "    print(f\"Image shape = {batch[\"pixel_values\"][0].shape}\")\n",
    "    \n",
    "    labels = batch[\"input_ids\"]\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "\n",
    "    print(\"Input IDs shape:\", input_ids.shape)\n",
    "    print(\"Attention mask shape:\", attention_mask.shape)\n",
    "    print(\"Pixel values shape:\", pixel_values.shape)\n",
    "    print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_collate_fn(examples):\n",
    "    # we only feed the prompt to the model\n",
    "    images = []\n",
    "    texts = []\n",
    "    answers = []\n",
    "    print(f\"eval_collate_fn: Number of examples: {len(examples)}\")\n",
    "    for example in examples:\n",
    "        image, ground_truth = example\n",
    "        if not isinstance(image, Image.Image):\n",
    "            raise ValueError(f\"Expected PIL Image, got {type(image)}\")\n",
    "        \n",
    "        images.append(image)\n",
    "        # messages = [\n",
    "        #     {\"role\": \"user\", \"content\": \"<image>\\nExtract JSON.\"},\n",
    "        #     {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        # ]\n",
    "        # prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "        # print(f\"Eval prompt: {prompt}\")\n",
    "        prompt = \"USER: <image>\\n Extract JSON\\n\" + \"ASSISTANT: \"\n",
    "        texts.append(prompt)\n",
    "        answers.append(ground_truth)\n",
    "\n",
    "    batch = processor(text=texts, \n",
    "                        images=images, \n",
    "                        padding=True,\n",
    "                        # truncation=True,\n",
    "                        # max_length=MAX_LENGTH,\n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": batch[\"input_ids\"],\n",
    "        \"attention_mask\": batch[\"attention_mask\"],\n",
    "        \"pixel_values\": batch[\"pixel_values\"],\n",
    "        \"answers\": answers  # Keep as list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387da47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class LlavaModelModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.batch_size = config.get(\"batch_size\", 8)\n",
    "        \n",
    "    def on_train_start(self):\n",
    "        print(f\"Training started\")\n",
    "        print(f\"Model config: {self.model.config}\")\n",
    "        print(f\"Processor config: {self.processor.tokenizer}\")\n",
    "        print(f\"Batch size: {self.batch_size}\")\n",
    "        print(f\"Training arguments: {self.config}\")\n",
    "        print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "        print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        print(f\"Training step {batch_idx}\")\n",
    "        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "        labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "        # print(f\"Forward pass:\")\n",
    "        # print(f\"Input IDs shape = {input_ids.shape}\")\n",
    "        # print(f\"Attention mask shape = {attention_mask.shape}\")\n",
    "        # print(f\"Pixel values shape = {pixel_values.shape}\")\n",
    "        # print(f\"Labels shape = {labels.shape}\")\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels,\n",
    "        )\n",
    "        # print(f\"Output shape: {outputs.logits.shape}\")\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def compute_score(self, pred: str, label: str) -> float:\n",
    "        try:\n",
    "            pred_json = json.loads(pred.strip())\n",
    "            label_json = json.loads(label.strip())\n",
    "            return 1.0 if pred_json == label_json else 0.0\n",
    "        except json.JSONDecodeError:\n",
    "            return 0.0\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"].to(self.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        pixel_values = batch[\"pixel_values\"].to(self.device)\n",
    "        answers = batch[\"answers\"]\n",
    "\n",
    "        generated_ids = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pixel_values=pixel_values,\n",
    "            max_new_tokens=MAX_LENGTH,\n",
    "        )\n",
    "        predictions = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        scores = []\n",
    "        for pred, label in zip(predictions, answers):\n",
    "            scores.append(self.compute_score(pred, label))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=5e-5)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        print(f\"train_dataloader called\")\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=train_collate_fn,\n",
    "        )\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        print(f\"val_dataloader called\")\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=eval_collate_fn,\n",
    "        )\n",
    "        return val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d557ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\": 10,\n",
    "          # \"val_check_interval\": 0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 8,\n",
    "          \"lr\": 1e-4,\n",
    "          \"batch_size\": 1,\n",
    "          # \"seed\":2022,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 50,\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c301d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_module = LlavaModelModule(config, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee11e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "WANDB_PROJECT = \"LLaVa\"\n",
    "WANDB_NAME = \"llava-demo-cord\"\n",
    "\n",
    "# wandb_logger = WandbLogger(project=WANDB_PROJECT, name=WANDB_NAME)\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[0],\n",
    "        max_epochs=config.get(\"max_epochs\"),\n",
    "        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        precision=\"16-mixed\",\n",
    "        limit_val_batches=5,\n",
    "        num_sanity_val_steps=0,\n",
    "        # logger=wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599acc32",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23231157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d44c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training checkpoint locally\n",
    "trainer.save_checkpoint(\"/home/mahadev/code/deepspeed/checkpoints/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9199355-317d-464a-9f63-ec680a8f79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push adapter weights to HuggingFace\n",
    "model_module.model.push_to_hub(\"mngaonkar/Llava-receipt-json\", commit_message=\"Training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09920fc-70c5-436e-bdd7-978bfa878fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapter weigths locally\n",
    "\n",
    "model_module.model.save_pretrained(\"/home/mahadev/code/deepspeed/adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1224dabb-35c6-4187-a7e2-2b4b67c7b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA or USE_QLORA:\n",
    "    if USE_QLORA:\n",
    "        print(\"Using QLoRA\")\n",
    "        # Load the model with 4-bit quantization\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        print(\"Using LoRA\")\n",
    "        bnb_config = None\n",
    "        \n",
    "    base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Using full precision\")\n",
    "    base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace53125-aef5-40bd-9e14-5e68b1c1d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters import AutoAdapterModel\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b725e-842c-4dbc-bf6c-56cfd7f757bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PEFT adapter\n",
    "adapter_path = \"/home/mahadev/code/deepspeed/adapter\"  # Local path or Hugging Face Hub repo\n",
    "peft_model = PeftModel.from_pretrained(base_model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b457342-3196-4a55-a412-977c2e7a3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b9f4b-b9ff-4065-b94f-26c6c0595b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_model.save_pretrained(\"./fused_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88dea0-b87f-486c-aa06-8ab6d1cf9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaForConditionalGeneration\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb0490-b9b1-4965-8ba9-1b7e53650f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_NAME,\n",
    "                                         torch_dtype=torch.float16,\n",
    "                                         use_auth_token=True)\n",
    "processor.tokenizer.padding_side = \"right\" # always on right for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85dba97-f5f3-419e-ac27-a70e075803c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4424e7f-a289-4599-9780-c33828b68f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\"mngaonkar/Llava-receipt-json\", torch_dtype=torch.float16, quantization_config=quantization_config).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275fcec-a8e4-4af3-be8c-94d95c1a81c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example = val_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7014a-dcba-48d2-a824-09a49712d3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74161e76-d3f8-47c4-aa4d-8ac018eeac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_example[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a22637-3691-4afb-804e-a97d7bb9739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"USER: <image>\\nExtract JSON\\n ASSISTANT: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aba078-774f-428b-b9ed-5adc31e53bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(text=prompt, images=[test_example[0]], padding=True, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2e9fbf-efb3-4609-9f77-df6a21545b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in inputs.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b4543-024a-4530-a531-377b319c4897",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed27c49-7bc6-4f24-8223-f8d882ea11f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265407b-cd89-4ef2-9974-b103620b7f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cf752-4fce-459b-beb2-dfb7ed85a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./fused_models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52033b79-aeb4-473b-a5e0-f1842075e31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e4bec8-f97a-40c5-9ec7-b09fa471bad6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
